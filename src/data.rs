use crate::boxes::xywh_to_x1y1x2y2;
use crate::config::{HEIGHT, WIDTH};
use crate::labels::SSDRemapCOCOID;
use crate::transforms::pipeline::Transform;
use crate::transforms::zoom_out::RandomZoomOut;
use burn::data::dataset::vision::{Annotation, BoundingBox, ImageDatasetItem, PixelDepth};
use burn::tensor::{Device, Element, Tensor, TensorData, backend::Backend};
use burn::{data::dataloader::batcher::Batcher, prelude::*};
use image::{DynamicImage, GrayImage, RgbImage};
use std::sync::{Arc, Mutex};

#[derive(Clone)]
pub enum BatchType {
    Train,
    Test,
}

#[derive(Clone)]
pub struct SSDBatcher<B: Backend> {
    cr: SSDRemapCOCOID,
    batch_id: Arc<Mutex<usize>>,
    batch_type: BatchType,
    phantom: std::marker::PhantomData<B>,
}

impl<B: Backend> SSDBatcher<B> {
    pub fn new(cr: &SSDRemapCOCOID, batch_type: BatchType) -> Self {
        Self {
            cr: cr.clone(),
            batch_id: Arc::new(Mutex::new(0)),
            batch_type,
            phantom: std::marker::PhantomData,
        }
    }

    /// Applies a series of image and annotation transformations depending on the batch type
    /// (Train or Test).
    ///
    /// # Parameters
    /// - `image: Tensor<B, 3>`: The input image tensor with 3 channels (RGB).
    /// - `bboxes: Option<Tensor<B, 2>>`: Optional bounding boxes associated with the
    ///    image, shape `[num_boxes, 4]`.
    /// - `labels: Option<Tensor<B, 1, Int>>`: Optional class labels corresponding to each
    ///    bounding box, shape `[num_boxes, 1]`.
    ///
    /// # Returns
    /// - `Result<(Tensor<B, 3>, Option<Tensor<B, 2>>, Option<Tensor<B, 1, Int>>), String>`:
    ///   Returns the transformed image, optionally transformed bounding boxes and labels, or an
    ///   error string.
    ///
    /// # Behavior
    /// - If `self.batch_type` is `BatchType::Train`, the following transformations are applied:
    ///   1. Random photometric distortion: brightness, contrast, saturation, and hue adjustments.
    ///   2. Random IoU-based cropping with minimum IoU threshold 0.5.
    ///   3. Random zoom-out with padding value 127, zoom range `(1.0, 4.0)` and probability 0.5.
    ///   4. Random horizontal flip with probability 0.5.
    ///   5. Resize the image to fixed dimensions `WIDTH x HEIGHT` using bilinear interpolation.
    ///   6. Remove invalid or degenerate bounding boxes (`clean_boxes`).
    ///   7. Normalize the image.
    ///   8. Return the final transformed tensors.
    ///
    /// - If `self.batch_type` is `BatchType::Test`, only deterministic preprocessing is applied:
    ///   1. Resize the image to `WIDTH x HEIGHT`.
    ///   2. Clean bounding boxes.
    ///   3. Normalize the image.
    ///   4. Return the final tensors.
    ///
    /// # Errors
    /// Returns a `String` error if `clean_boxes()` or any other step in the pipeline fails.
    ///
    /// Relevant excerpt from:
    ///
    /// “SSD: Single Shot MultiBox Detector”
    /// Authors: Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,
    ///          Scott Reed, Cheng-Yang Fu, Alexander C. Berg
    /// Link (official): https://arxiv.org/abs/1512.02325
    ///
    /// Data Augmentation for Small Object Accuracy - Liu et al. - Pg 12
    ///
    /// Without a follow-up feature resampling step as in Faster R-CNN, the classification
    /// task for small objects is relatively hard for SSD, as demonstrated in our analysis (see
    /// Fig. 4).The data augmentation strategy described in Sec. 2.2 helps to improve the
    /// performance dramatically, especially on small datasets such as PASCAL VOC. The random
    /// crops generated by the strategy can be thought of as a ”zoom in” operation and can
    /// generate many larger training examples. To implement a ”zoom out” operation that
    /// creates more small training examples, we first randomly place an image on a canvas of 16×
    /// of the original image size filled with mean values before we do any random crop
    /// operation. Because we have more training images by introducing this new ”expansion”
    /// data aug-mentation trick, we have to double the training iterations. We have seen a
    /// consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6. In
    /// specific, Fig-ure 6 shows that the new augmentation trick significantly improves the
    /// performance on small objects. This result underscores the importance of the data
    /// augmentation strategy for the final model accuracy.
    ///
    fn process_transformations(
        &self,
        image: Tensor<B, 3>,
        bboxes: Option<Tensor<B, 2>>,
        labels: Option<Tensor<B, 1, Int>>,
    ) -> Result<
        (
            Tensor<B, 3>,
            Option<Tensor<B, 2>>,
            Option<Tensor<B, 1, Int>>,
        ),
        String,
    > {
        match self.batch_type {
            BatchType::Train => Transform::from_tensors(image, bboxes, labels)
                .random_photometric_distort((-0.40, 0.40), (-0.40, 0.40), (-70.0, 70.0), 0.5)
                .random_iou_crop(0.5)
                .random_zoom_out(127, (1.0, 4.0), 0.5)
                .random_horizontal_flip(0.5)
                .resize_bilinear(WIDTH, HEIGHT)
                .clean_boxes()?
                .normalize()
                .finish(),
            BatchType::Test => Transform::from_tensors(image, bboxes, labels)
                .resize_bilinear(WIDTH, HEIGHT)
                .clean_boxes()?
                .normalize()
                .finish(),
        }
    }
}

#[derive(Clone, Debug)]
pub struct SSDBatch<B: Backend> {
    pub images: Tensor<B, 4>,
    pub gt_boxes: Tensor<B, 3>,
    pub target_labels: Tensor<B, 2, Int>,
    pub target_padding: Tensor<B, 2, Int>,
    pub batch_ids: Vec<usize>,
}

/// Converts raw image data into a normalized tensor in channel-first format.
///
/// # Type Parameters
/// - `B`: Backend type (CPU/GPU)
/// - `T`: Element type of the input data (e.g., u8, f32)
///
/// # Arguments
/// - `data`: Flattened image data in row-major order ([H * W * C])
/// - `shape`: Image shape as [height, width, channels]
/// - `device`: Target device for the tensor
///
/// # Returns
/// A 3D tensor with shape [C, H, W], normalized to [0.0, 1.0]
pub fn to_tensor<B: Backend, T: Element>(
    data: Vec<T>,
    shape: [usize; 3],
    device: &Device<B>,
) -> Tensor<B, 3> {
    Tensor::<B, 3>::from_data(
        TensorData::new(data, shape).convert::<B::FloatElem>(),
        device,
    )
    .permute([2, 0, 1])
    .div_scalar(255)
}

impl<B: Backend> Batcher<B, ImageDatasetItem, SSDBatch<B>> for SSDBatcher<B> {
    fn batch(&self, items: Vec<ImageDatasetItem>, device: &B::Device) -> SSDBatch<B> {
        // Create tensor from image data

        let mut image_batch = Vec::<Tensor<B, 3>>::new();
        let mut label_batch = Vec::<Tensor<B, 1, Int>>::new();
        let mut bbox_batch = Vec::<Tensor<B, 2>>::new();
        let mut padding_batch = Vec::<Tensor<B, 1, Int>>::new();
        let mut batch_ids = vec![];

        let mut max_targets = 0;

        // Filter items that are not defined in the SSD remap struct

        let items: Vec<ImageDatasetItem> = items
            .iter()
            .map(|item| {
                let mut item = item.clone();

                item.annotation = match &item.annotation {
                    Annotation::BoundingBoxes(bboxes) => Annotation::BoundingBoxes(
                        bboxes
                            .clone()
                            .iter_mut()
                            .map(|bbox| bbox.clone())
                            .filter(|an| self.cr.contains(an.label))
                            .map(|bbox| BoundingBox {
                                coords: bbox.coords,
                                label: self.cr.coco_id_to_model_id(&bbox.label).unwrap(),
                            })
                            .collect(),
                    ),
                    _ => panic!("No Bounding Boxes Found For Image!"),
                };

                let annotations = match &item.annotation {
                    Annotation::BoundingBoxes(items) => items,
                    _ => panic!("No Bounding Boxes Found For Image after filtering!"),
                };

                max_targets = if max_targets < annotations.len() {
                    annotations.len()
                } else {
                    max_targets
                };

                item
            })
            .collect();

        for item in items {
            // Add a unique id to each batch for debug purposes

            {
                let mut cur_id = self.batch_id.lock().unwrap();
                *cur_id += 1;
                batch_ids.push(*cur_id);
            }

            let annotations = match item.clone().annotation {
                Annotation::BoundingBoxes(items) => items,
                _ => panic!("No Bounding Boxes Found For Image!"),
            };

            let bboxes: Tensor<B, 2> = Tensor::cat(
                annotations
                    .iter()
                    .map(|ann| Tensor::<B, 1>::from_data(ann.coords.as_slice(), device))
                    .collect(),
                0,
            )
            .reshape([-1, 4]);

            // Convert from coco format coords
            let bboxes = xywh_to_x1y1x2y2(bboxes);

            let labels = Tensor::cat(
                annotations
                    .iter()
                    .map(|f| Tensor::from_ints([f.label as i64], device))
                    .collect(),
                0,
            );
            let image = item.clone().to_rgb_tensor(device);

            let result = self.process_transformations(
                image.clone(),
                Some(bboxes.clone()),
                Some(labels.clone()),
            );

            let (image, bboxes, labels) = if let Ok(transformed_img) = result {
                transformed_img
            } else {
                println!(
                    "Error: {} using original input params.",
                    result.unwrap_err()
                );
                Transform::from_tensors(image, Some(bboxes), Some(labels))
                    .resize_bilinear(HEIGHT, WIDTH)
                    .normalize()
                    .finish()
                    .unwrap()
            };

            let bboxes = bboxes.unwrap().split(1, 0);
            let labels = labels.unwrap().split(1, 0);

            let (bboxes, labels, padding) = pad(bboxes, labels, max_targets);

            image_batch.push(image);
            bbox_batch.push(bboxes);
            label_batch.push(labels);
            padding_batch.push(padding);
        }

        let images = Tensor::stack(image_batch, 0);
        let target_labels = Tensor::stack(label_batch, 0);
        let gt_boxes = Tensor::stack(bbox_batch, 0);
        let target_padding = Tensor::stack(padding_batch, 0);

        SSDBatch {
            images,
            target_labels,
            gt_boxes,
            target_padding,
            batch_ids: batch_ids.clone(),
        }
    }
}

/// Pads ground-truth bounding boxes and target labels to a fixed length to prepare for batching
///
/// # Arguments
/// - `gt_boxes`: Vector of 2D tensors, each representing ground-truth boxes for an image
///   ([num_boxes, 4])
/// - `target_labels`: Vector of 1D tensors, each containing class labels for corresponding boxes
/// - `padding_len`: Target number of boxes after padding
///
/// # Returns
/// A tuple containing:
/// 1. `Tensor<B, 2>`: concatenated and padded ground-truth boxes
/// 2. `Tensor<B, 1, Int>`: concatenated and padded target labels
/// 3. `Tensor<B, 1, Int>`: tensor indicating how many padding entries were added
///
pub fn pad<B: Backend>(
    gt_boxes: Vec<Tensor<B, 2>>,
    mut target_labels: Vec<Tensor<B, 1, Int>>,
    padding_len: usize,
) -> (Tensor<B, 2>, Tensor<B, 1, Int>, Tensor<B, 1, Int>) {
    let device = &gt_boxes[0].device();

    if gt_boxes.len() != target_labels.len() {
        panic!("Target boxes must match target labels lengths");
    }

    if gt_boxes.is_empty() {
        panic!("Target boxes length cannot be zero");
    }

    let padding_len = padding_len - gt_boxes.len();

    let mut gt_boxes = Tensor::cat(gt_boxes, 0);

    if padding_len > 0 {
        gt_boxes = gt_boxes.pad((0, 0, 0, padding_len), 0);
        target_labels.push(Tensor::zeros([padding_len], device));
    }

    let target_labels = Tensor::cat(target_labels, 0);

    let target_padding = Tensor::<B, 1, Int>::from_ints([padding_len], device);

    (gt_boxes, target_labels, target_padding)
}

/// Removes padding from ground-truth boxes and target labels
///
/// # Arguments
/// - `gt_boxes`: A 2D tensor of ground-truth bounding boxes, possibly including padded entries
/// - `target_labels`: A 1D tensor of class labels corresponding to `gt_boxes`, possibly including
///   padding
/// - `target_padding`: A 1D tensor indicating how many padding entries were added
///
/// # Returns
/// A tuple containing:
/// 1. `Tensor<B, 1, Int>`: target labels with padding removed
/// 2. `Tensor<B, 2>`: ground-truth boxes with padding removed
pub fn strip_padding<B: Backend>(
    gt_boxes: Tensor<B, 2>,
    target_labels: Tensor<B, 1, Int>,
    target_padding: Tensor<B, 1, Int>,
) -> (Tensor<B, 1, Int>, Tensor<B, 2>) {
    // strip padding from labels and boxes

    let target_padding = target_padding.to_data().to_vec::<i64>().unwrap()[0];

    let [items_length] = target_labels.shape().dims();
    let items_length = items_length as i64 - target_padding;
    let target_labels = target_labels.slice(s![0..items_length]);
    let gt_boxes = gt_boxes.slice(s![0..items_length, ..]);

    (target_labels, gt_boxes)
}

pub trait ItemToRgbImage {
    fn to_dynamic_image(&self) -> Option<DynamicImage>;
    fn to_rgb_tensor<B: Backend>(self, device: &<B as Backend>::Device) -> Tensor<B, 3>;
}

impl ItemToRgbImage for ImageDatasetItem {
    fn to_dynamic_image(&self) -> Option<DynamicImage> {
        //Result<String, io::Error> {
        let raw_image: Vec<u8> = self
            .image
            .iter()
            .map(|f| {
                if let PixelDepth::U8(p) = *f {
                    p
                } else {
                    panic!("Expecting type PixelDepth::U8")
                }
            })
            .collect();

        if let Some(val) = RgbImage::from_raw(
            self.image_width as u32,
            self.image_height as u32,
            raw_image.clone(),
        ) {
            return Some(DynamicImage::ImageRgb8(val));
        };

        if let Some(val) =
            GrayImage::from_raw(self.image_width as u32, self.image_height as u32, raw_image)
        {
            return Some(DynamicImage::ImageLuma8(val));
        };

        None
    }

    fn to_rgb_tensor<B: Backend>(self, device: &<B as Backend>::Device) -> Tensor<B, 3> {
        let raw_image: Vec<u8> = self
            .image
            .iter()
            .map(|f| {
                if let PixelDepth::U8(p) = *f {
                    p
                } else {
                    panic!("Expecting type PixelDepth::U8")
                }
            })
            .collect();

        // Check for Luma 8 (greyscale) convert to RGB

        if raw_image.len() == self.image_height * self.image_width {
            Tensor::<B, 1>::from_data(raw_image.repeat(3).as_slice(), device).reshape([
                3,
                self.image_height,
                self.image_width,
            ])
        } else {
            let data = TensorData::new(raw_image, [self.image_height, self.image_width, 3]);
            let data = data.convert::<B::FloatElem>();

            // [H, W, C] -> [C, H, W]
            Tensor::<B, 3>::from_data(data, device).permute([2, 0, 1])
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use burn::backend::NdArray;
    use burn::backend::ndarray::NdArrayDevice;
    use burn::tensor::{Tolerance, ops::FloatElem};

    #[test]
    fn test_pad() {
        let device = &NdArrayDevice::default();
        type B = NdArray<f32>;
        let padding = 6;

        let target_labels1 = Tensor::<B, 1, Int>::from_data([11, 8, 8], device);
        let target_labels2 = Tensor::<B, 1, Int>::from_data([11], device);
        let gt_bxes1 = Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916], // 0
            ],
            device,
        );

        let gt_bxes2 = Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916], // 0
                [0.35725, 0.51429164, 0.61651564, 0.7677916], // 0
                [0.35725, 0.51429164, 0.61651564, 0.7677916], // 0
            ],
            device,
        );

        let (a, b, c) = pad(
            vec![gt_bxes2, gt_bxes1],
            vec![target_labels1, target_labels2],
            padding,
        );

        assert!(a.dims() == [8,4]);
        assert!(b.dims() == [8]);
        assert!(c.dims() == [1]);
    }

    #[test]
    fn test_pad_single() {
        let device = &NdArrayDevice::default();
        type B = NdArray<f32>;
        type FT = FloatElem<B>;

        let padding = 6;
        let target_label = Tensor::<B, 1, Int>::from_data([11], device);
        let gt_boxes = Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916], // 0
            ],
            device,
        );

        let (a, b, c) = pad(vec![gt_boxes], vec![target_label], padding);

        Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
            ],
            device,
        )
        .into_data()
        .assert_approx_eq::<FT>(&a.to_data(), Tolerance::default());

        Tensor::<B, 1, Int>::from_data([11, 0, 0, 0, 0, 0], device)
            .into_data()
            .assert_approx_eq::<FT>(&b.to_data(), Tolerance::default());

        Tensor::<B, 1, Int>::from_data([5], device)
            .into_data()
            .assert_approx_eq::<FT>(&c.to_data(), Tolerance::default());
    }

    #[test]
    fn test_batch_strip_padding() {
        let device = &NdArrayDevice::default();
        type B = NdArray<f32>;
        type FT = FloatElem<B>;

        let gt_boxes = Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 0.0],
            ],
            device,
        );

        let target_labels = Tensor::<B, 1, Int>::from_data([11, 8, 8, 11, 0, 0, 0, 0], device);

        let target_padding = Tensor::<B, 1, Int>::from_data([4], device);

        let (a, b) = strip_padding(gt_boxes, target_labels, target_padding);

        Tensor::<B, 2>::from_data(
            [
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
                [0.35725, 0.51429164, 0.61651564, 0.7677916],
            ],
            device,
        )
        .into_data()
        .assert_approx_eq::<FT>(&b.to_data(), Tolerance::default());

        Tensor::<B, 1, Int>::from_data([11, 8, 8, 11], device)
            .into_data()
            .assert_approx_eq::<FT>(&a.to_data(), Tolerance::default());
    }
}
